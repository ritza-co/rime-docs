---
title: Quickstart
---

<Tip>On-prem is  now **public and generally available!** For more information regarding access to Docker images and pricing info, reach out to help@rime.ai.</Tip>

# Introduction

## Why on-premises?

Deploying on-premises offers several advantages over using cloud APIs over a public network. One of the main benefits is speed; by hosting the services locally, you can significantly reduce network latency, resulting in faster system responses and data processing.

## Security

With an on-premises deployment, all sensitive data remains within your corporate network, ensuring enhanced security as it is not transmitted over the Internet. This setup helps in complying with strict data privacy and protection regulations.

## Performance

### Latency

- **Mistv2:** Our tests have shown median latency of **175ms** with randomly generated sentences between 40 and 50 characters on A10Gs and similar GPUs.
- **Arcana:** See [performance tuning](/docs/on-prem/performance#metrics).

## Components

![On-Premise Components](/images/on-prem-components_update.png)

# Prerequisites

## Hardware requirements

- GPU
  - For Mist
    - NVIDIA T4, L4, A10, or higher
  - For Arcana
    - NVIDIA A100, H100 MIG `3g.40gb`, or higher
- Storage
  - 50 GB storage
- CPU
  - 8 vCPUs
- Memory requirements
  - 32 GiB

## Software requirements

- Supported Linux Distributions
  - Debian 12 (`bookworm`), x86_64
  - Ubuntu Server 24.04 (`jammy`), x86_64
- NVIDIA drivers
  - Minimum: `525.60.13`
  - Recommended: `570.133.20` or higher
- Docker
- NVIDIA Container Toolkit

### Installations

#### NVIDIA drivers

Follow https://www.nvidia.com/en-us/drivers to install the latest NVIDIA drivers, or use the following instructions on Debian-based systems:

```bash NVIDIA Driver Installation (Debian-based)
# Update packages
sudo apt-get update

# Install basic toolchain and kernel headers
sudo apt-get install -y gcc make wget linux-headers-$(uname -r)

# Download and install the NVIDIA driver.
NVIDIA_DRIVER_VERSION=580.95.05
NVIDIA_DRIVER_PATH=/opt/NVIDIA-Linux-x86_64-${NVIDIA_DRIVER_VERSION}.run
sudo rm -f "${NVIDIA_DRIVER_PATH}"
sudo wget "https://us.download.nvidia.com/tesla/${NVIDIA_DRIVER_VERSION}/NVIDIA-Linux-x86_64-${NVIDIA_DRIVER_VERSION}.run" -O "${NVIDIA_DRIVER_PATH}"
sudo chmod +x "${NVIDIA_DRIVER_PATH}"
sudo "${NVIDIA_DRIVER_PATH}" --silent --no-questions
```

#### Docker

Follow https://docs.docker.com/engine/install to install Docker on your system.

Optionally, add the current user to the `docker` group for convenience: https://docs.docker.com/engine/install/linux-postinstall.
The code snippets below assume that you can run `docker` as the current login.

#### NVIDIA Container Toolkit

Follow https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html to install the NVIDIA Container Toolkit.

Note that you should follow both the **Installation** and the **Configuration** sections.

#### Verification

To verify that you have all the prerequisites installed, run the following command:

```bash Verify Prerequisites
docker run --rm --gpus all nvidia/cuda:12.8.1-base-ubi9 nvidia-smi
```

You should see your GPU listed in the output, alongside the driver version and CUDA version.

## Firewall requirements

The Rime API instance will listen on port 8000 for HTTP traffic, and on port 8001 for WebSocket traffic.

You will also need to allow the following outbound traffic in your firewall rules:

- `https://optimize.rime.ai/usage`: registers on-prem usage with our servers.
- `https://optimize.rime.ai/license`: verifies that your on-prem license is active.
- `us-docker.pkg.dev` on port 443: container image registry.

# Self-service licensing and credentials

## API key Generation

Refer to our [user interface dashboard](https://rime.ai/dashboard) to generate the necessary keys and credentials for authenticating and authorizing the deployment and use of our services.

# Deployment

The deployment consists of two services, each powered by a container image:

- **API service:** responsible for handling the HTTP and WebSocket requests, and for verifying the license. It serves as a proxy to the TTS service.
- **TTS service:** responsible for model inference.

There is a 1:1 relationship between the API service and the TTS service: for each TTS model, you will need a corresponding API service. Multiple pairs of API and TTS services can be deployed on the same machine.

## Artifact Registry login

Key file to be provided by Rime.

```bash Log in to Artifact Registry
cat KEY-FILE | docker login -u _json_key --password-stdin https://us-docker.pkg.dev
```

## Container images

### TTS service

#### Arcana

The Arcana images can be found at `us-docker.pkg.dev/rime-labs/arcana/v2/<language>:<tag>`.

* The support languages are: `ar`, `de`, `en`, `es`, `fr`, `hi`, `si`.
* The latest version is `20260124`.

For Arcana only, you can also load the engine and data packages from different containers:

* `us-docker.pkg.dev/rime-labs/engine/arcana:<tag>`
* `us-docker.pkg.dev/rime-labs/package/arcana/<language>:<tag>`

#### Mist

There is only a Mist on-prem image for English:

* `us-docker.pkg.dev/rime-labs/mist/v2/en:20251106`

### API service

The latest image version is:

* `us-docker.pkg.dev/rime-labs/api/service:20260123`

## Docker Compose configuration

A simple way of deploying on a machine is to use Docker Compose.

Create a `compose.yml` file with your editor of choice to define the services and their configurations:

```yaml compose.yml
version: '3.8'
services:
  api:
    image: us-docker.pkg.dev/rime-labs/api/service:<tag>
    depends_on:
      - model
    ports:
      - "8000:8000"  # HTTP API
      - "8001:8001"  # Json WebSocket API
      - "8002:8002"  # Json WebSocket API
    restart: unless-stopped
    environment:
      - MODEL_URL=http://model:8080/invocations

  model:
    image: us-docker.pkg.dev/rime-labs/<model>/<version>/<language>:<tag>
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    restart: unless-stopped
```

> When running on Kubernetes, ensure that `MODEL_URL` points to `http://0.0.0.0:8080/invocations` instead of the Docker Compose service name.

### Multi-model backend

If you want to serve multiple Arcana languages via a single API instance, you can create a `compose.yml` like the following:

```yaml compose.yml
services:
  en-api: us-docker.pkg.dev/rime-labs/api/service:<tag>
    image:
    depends_on:
      - en-model
      - es-model
    ports:
      - "8000:8000"
    restart: unless-stopped
    environment:
      - MODEL_URL=http://en-model:8080/invocations
      - ARCANA_ENG_MODEL_URL=http://en-model:8080/invocations
      - ARCANA_SPA_MODEL_URL=http://es-model:8080/invocations
  en-model:
    image: us-docker.pkg.dev/rime-labs/arcana/v2/en:<tag>
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    restart: unless-stopped

  es-model:
    image: us-docker.pkg.dev/rime-labs/arcana/v2/es:<tag>
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    restart: unless-stopped
```

Note that the `ARCANA_{LANG}_MODEL_URL` environment variable must point to the container running the Arcana image for that language,
but you should still point `MODEL_URL` to a default model container. The model environment variables currently supported are:

```
ARCANA_ENG_MODEL_URL
ARCANA_SPA_MODEL_URL
ARCANA_FRA_MODEL_URL
ARCANA_GER_MODEL_URL
ARCANA_ARA_MODEL_URL
```

The API will route to these model backends based on the request parameter <a href="https://docs.rime.ai/api-reference/arcana/streaming-mp3#param-lang">lang</a>.

### Start Docker Compose

```bash Start Docker Compose
docker compose up -d
```

## Deployment steps

<iframe src="https://drive.google.com/file/d/1zzrPCVIDsiTMNY_pyb4Z2TezgCTl4Qc1/preview" width="560" height="315" allow="autoplay" allowfullscreen></iframe>

1. **Environment setup:** Prepare your AWS environment according to the specifications required for optimal deployment.
2. **Service deployment:** Using Docker, deploy the images on your server.
3. **Networking setup:** Configure the network settings, including the Internet Gateway and port settings, to ensure connectivity and security.
4. **Licensing and authentication:** Generate and apply the necessary API key via our dashboard to start using the services.

> **Note:** Once the containers are started, expect a five-minute delay for warm-up before sending the first TTS requests.

## Additional information

- **Troubleshooting guide:** A troubleshooting guide will be provided to help resolve common issues during deployment.
- **Available voices and models:** All voices are currently available.

# Requests and response formats

## HTTP requests

**Request:**

```bash Health check
curl http://localhost:8000/health
```
This should return:
```json
{
    "apiStatus":"ok",
    "timestamp":timestamp,
    "licenseStatus":"valid"/"expired-or-not-set",
    "modelReachable":true/false
}
```

```bash Request example
curl -X POST "http://localhost:8000" -H "Authorization: Bearer <API KEY> -H "Content-Type: application/json" -d '{
  "text": "I would love to have a conversation with you. The new model is out.",
  "speaker": "joy",
  "modelId": "mist"
}' -o result_mist.txt
```

**Response:**

```json Response format
{"audioContent":{"model_output":"<base64>"}}
```

Sample response file: [`result.txt`](https://drive.google.com/file/d/1GW2D8pm5witYMQdKQrPvp_OW5PM0TxNj/view?usp=drive_link)

## Receiving a response in MP3 format

**Request:**

```bash Request example
curl -X POST "http://localhost:8000" -H "Authorization: Bearer <API KEY>" -H "Content-Type: application/json" -H "Accept: audio/mp3" -d '{
  "text": "I would love to have a conversation with you.",
  "speaker": "joy",
  "modelId": "mist"
}' -o result.mp3
```

**Response:**

Sample response file: [`result.mp3`](https://drive.google.com/file/d/1iwmWB1byBXknmNvNmvB_SgBpvnU4bChJ/view?usp=sharing)

### Receiving a response in PCM (raw) format

**Request:**

```bash Request example
curl -X POST "http://localhost:8000" -H "Authorization: Bearer <API KEY>" -H "Content-Type: application/json" -H "Accept: audio/pcm" -d '{
  "text": "I would love to have a conversation with you.",
  "speaker": "joy",
  "modelId": "mist"
}' -o result.pcm
```

**Response:**

Sample response file: [`result.pcm`](https://drive.google.com/file/d/1pwkGW9jCe1TN9GF5yQa6j619vc8WfjGu/view?usp=drive_link)

### Binary WebSocket endpoint

The binary (non-JSON) WebSocket endpoint will be served at port `8002`.
For example, `ws://localhost:8002`, which will be equivalent to our [cloud websockets API](/api-reference/endpoint/websockets).

### JSON WebSocket endpoint

The JSON WebSocket endpoint will be served at port `8001`.
For example, `ws://localhost:8001`, which will be equivalent to our [cloud websockets-json API](/api-reference/endpoint/websockets-json).
