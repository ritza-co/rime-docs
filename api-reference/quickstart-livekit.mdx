---
title: LiveKit Quickstart
icon: rocket
---

This guide demonstrates how to build a real-time voice agent using LiveKit's Python SDK with natural speech provided by Rime. The agent listens to you using LiveKit's plugins: `silero` and `turn-detector`, for conversational turn-taking. It synthesizes your speech using `gpt-4o-transcribe` and generates responses using `gpt-4o-mini`. Then, it generates realistic text-to-speech (TTS) using Rime's API. By the end, you'll have a working voice agent that you can talk to in your browser. 

<img
  src="/images/livekit-agent-flow-light.png"
  alt="Voice agent architecture showing audio flowing from user through LiveKit, OpenAI, and Rime"
  className="block dark:hidden"
/>
<img
  src="/images/livekit-agent-flow-dark.png"
  alt="Voice agent architecture showing audio flowing from user through LiveKit, OpenAI, and Rime"
  className="hidden dark:block"
/>

If you'd like to experiment with Rime's TTS API directly before building a full voice agent, check out the [Python Quickstart](/api-reference/quickstart-python).

## Prerequisites

- **[Rime API Key](https://app.rime.ai/tokens/)** for text-to-speech synthesis
- **[OpenAI API Key*](https://platform.openai.com/api-keys)** for speech-to-text and LLM responses (see [Alternative Providers](#alternative-providers) for using Deepgram and Gemini instead)
- **[LiveKit Cloud Account](https://cloud.livekit.io/)** for real-time audio transport
    - Create a new LiveKit project called `rime-agent`
    - Go to **Settings** -> **API keys** -> **Create key** to generate a **WebSocket URL**, **API key**, and **API secret**. You will need all three of these
- **Python (3.10+):** the language to put it all together

## Project setup

Create a new folder for your project and navigate into it:

```bash
mkdir rime-voice-agent
cd rime-voice-agent
```

### Set up the environment variables

In the new directory, create a file called `.env` to store your API keys securely:

```bash
touch .env
```

Open `.env` and add your keys:

```
LIVEKIT_URL=wss://your-project.livekit.cloud (this will be the )
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
OPENAI_API_KEY=your_openai_api_key
RIME_API_KEY=your_rime_api_key
```

### Configure the dependencies

Install `uv` which is a Python package manager to handle dependencies:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Create a file called `pyproject.toml` that defines your project and its dependencies and add the following:

```toml
[project]
name = "rime-voice-agent"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
    "livekit-agents[openai,rime,silero,turn-detector]~=1.2",
    "livekit-plugins-noise-cancellation~=0.2",
    "python-dotenv>=1.1.1",
]
```

- **openai plugin**: Connects to OpenAI for speech-to-text and LLM
- **rime plugin**: Connects to Rime for text-to-speech
- **silero plugin**: Voice Activity Detection (knows when you start/stop speaking)
- **turn-detector plugin**: Detects when you've finished your turn in the conversation
- **noise-cancellation plugin**: Filters out background noise
- **python-dotenv** - Loads your API keys from the `.env` file

Now, start a virtual environment and download the dependencies by running:

```bash
uv sync
```

## Create the Agent

Create a file called `agent.py` for all the code to get your agent talking. 

### Load environment variables

First, load the environment variables so you can use them throughout the application:

```python
from dotenv import load_dotenv

load_dotenv()
```

### Define an Agent class
Now it's time to get stuck-in with the LiveKit SDK.
Define your agent's personality by creating a class that extends `livekit.agents.Agent`:

```python
class Assistant(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a helpful voice assistant. 
            Keep your responses short and conversational - no more than 2-3 sentences.
            Be friendly and natural."""
        )
```

Initialize the class by giving it a system prompt that will guide your agent's responses and personality. This prompt can be as simple or complex as you like. Later in the guide (see [Fine-tune Agent personalities](#fine-tune-agent-personalities)) you will see an example of a detailed system prompt that will fully customize the agent's behavior.

Remember to import the `livekit.agents` dependency. For convenience, you can find the complete list of required dependencies for this guide below:

```python
from dotenv import load_dotenv
from livekit.agents import (
    Agent,
    AgentSession,
    AutoSubscribe,
    JobContext,
    JobProcess,
    RoomInputOptions,
    WorkerOptions,
    cli,
)
from livekit.plugins import openai, noise_cancellation, rime, silero
from livekit.plugins.turn_detector.multilingual import MultilingualModel
```
### Code the conversation pipeline 

The entrypoint function runs each time a user connects. First, connect to the LiveKit room and wait for a participant to join:

```python
async def entrypoint(ctx: JobContext):
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    
    await ctx.wait_for_participant()
```

Next, in the same function, create the `AgentSession` which wires together all the components of your voice pipeline. This is where you specify which models to use for each stage:

```python  
    session = AgentSession(
        stt=openai.STT(model="gpt-4o-transcribe"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=rime.TTS(model="arcana", speaker="luna"),
        vad=ctx.proc.userdata["vad"],
        turn_detection=MultilingualModel(),
    )
```

Continue the function by starting the session and attaching it to the room. The `noise_cancellation` option filters out background noise from the user's microphone:

```python
    await session.start(
        room=ctx.room,
        agent=Assistant(),
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC()
        ),
    )
```

Finally, configure the agent to start the session with a greeting:

```python
    await session.say("Hey there! How can I help you today?")
```
### Initialize the VAD plugin

Create a `prewarm` function that loads the Voice Activity Detection plugin once when the worker starts. This is broken out into its own function to avoid calling it for each new conversation:

```python
def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()
```

### Create the main entrypoint

Finally, create the `__main__` block that runs when we execute the script:

```python
if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
        ),
    )
```

The `cli.run_app` call starts the agent server which listens for incoming connections from LiveKit. `WorkerOptions` is configured with the two functions you created above: `prewarm_fnc` runs once per worker process to preload models, and `entrypoint_fnc` runs each time a user connects to a room.

## Test your agent

Start your agent by running:

```bash
uv run python agent.py dev
```

The `dev` argument starts the agent in development mode. You'll see output like:

```
INFO   livekit.agents     starting worker {"version": "1.3.12", ...}
INFO   livekit.agents     initializing process {"pid": 12345, ...}
INFO   livekit.agents     process initialized
INFO   livekit.agents     HTTP server listening on :12332
```

### Connect to your Agent

Open the [LiveKit Agents Playground](https://agents-playground.livekit.io) in your browser.

- Select your project and click **Use [project_name]**
- In the top-right of the Playground, click **Connect**
- Allow microphone access when prompted

If everything is set up correctly, you should hear your agent say the greeting that you configured above:

```python
    await session.say("Hey there! How can I help you today?")
```

You can now talk to your agent using your microphone or by typing in the chat section.

### Change the voice

Rime offers many voices. Update the `tts` line to try different ones:

```python
tts=rime.TTS(model="arcana", speaker="orion"),
```

See more of the available voices [here](/api-reference/voices).

### Fine-tune Agent personalities

For more complex personalities, store your system prompt in a separate file. Create a new file called `personality.py`. See the example below for inspiration on how you could customize your agent:

<Accordion title="Example personality.py">
```python
SYSTEM_PROMPT = """
CHARACTER:
You are Gary, an overly enthusiastic tech support agent who is convinced 
that every problem can be solved by turning it off and on again. You work 
for a fictional company called "Reboot Solutions" and take your job very seriously.

PERSONALITY:
- Extremely optimistic, even when things are clearly going wrong
- Obsessed with the power of rebooting
- Uses tech jargon incorrectly but confidently
- Gets genuinely excited when users describe their problems
- Occasionally references "the great outage of 2019" as a formative experience

SPEECH STYLE:
- Keep responses to 2-3 sentences maximum
- Add natural fillers like "um", "well", "you know"
- Express enthusiasm with phrases like "Oh, fantastic!" and "Love it!"
- Always end with a question to keep the conversation going

RESTRICTIONS:
- Never break character
- Don't use emojis or special characters
- Avoid technical accuracy - confidence matters more than correctness
"""

INTRO_MESSAGE = "Oh hey there! Welcome to Reboot Solutions, I'm Gary! What can I help you restart today?"
```
</Accordion>

Then update your `agent.py` to import and use this prompt:

```python
from personality import SYSTEM_PROMPT, INTRO_MESSAGE

class Assistant(Agent):
    def __init__(self):
        super().__init__(instructions=SYSTEM_PROMPT)
```

And update the greeting at the end of `entrypoint`:

```python
await session.say(INTRO_MESSAGE)
```

This approach keeps your personality configuration separate from your agent logic, making it easy to experiment with different characters.

## Alternative providers

LiveKit's plugin system lets you swap out your models for other STT and LLM providers. For example, if you don't have an OpenAI subscription, here's how to use Deepgram for speech-to-text and Google Gemini for the LLM.

### Update dependencies

Replace the OpenAI plugin with Deepgram and Google plugins in your `pyproject.toml`:

```toml
dependencies = [
    "livekit-agents[deepgram,google,rime,silero,turn-detector]~=1.2",
    "livekit-plugins-noise-cancellation~=0.2",
    "python-dotenv>=1.1.1",
]
```

Run `uv sync` to install the new dependencies.

### Update Environment Variables

Replace `OPENAI_API_KEY` in your `.env` file with:

```
DEEPGRAM_API_KEY=your_deepgram_api_key
GOOGLE_API_KEY=your_google_api_key
```

Get your keys from:
- **Deepgram**: [console.deepgram.com](https://console.deepgram.com/)
- **Google AI Studio**: [aistudio.google.com/apikey](https://aistudio.google.com/)

Don't change your LiveKit or Rime variables.

### Update the Agent Code

Change the imports and `AgentSession` configuration in `agent.py`:

```python
from livekit.plugins import deepgram, google, noise_cancellation, rime, silero
```

```python
session = AgentSession(
    stt=deepgram.STT(),
    llm=google.LLM(model="gemini-2.0-flash"),
    tts=rime.TTS(model="arcana", speaker="luna"),
    vad=ctx.proc.userdata["vad"],
    turn_detection=MultilingualModel(),
)
```

Your agent now uses Deepgram for transcription and Gemini for responses while keeping Rime for natural-sounding speech.

## Troubleshooting

### Agent doesn't respond to speech

- **Check microphone permissions**: Ensure your browser has microphone access enabled
- **Verify VAD is working**: Look for "speech detected" logs in the terminal. If missing, check your Silero plugin installation
- **Test with text input**: Use the chat input in the Playground to confirm the agent logic works

### "Connection refused" or agent won't start

- **Check environment variables**: Ensure all keys in `.env` are set correctly with no extra spaces
- **Verify LiveKit credentials**: Confirm your `LIVEKIT_URL`, `LIVEKIT_API_KEY`, and `LIVEKIT_API_SECRET` match your LiveKit Cloud project

### Incorrect voice detection

- **Enable noise cancellation**: Verify `noise_cancellation.BVC()` is included in your `RoomInputOptions`
- **Check your microphone**: Test with a different input device or headset
- **Reduce background noise**: The VAD may struggle to detect speech in noisy environments
