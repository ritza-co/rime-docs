---
title: LiveKit Quickstart
icon: rocket
---

This guide demonstrates how to build a real-time voice agent using LiveKit's Agents SDK with natural speech provided by Rime. The agent uses LiveKit's plugins:
-  `silero` and `turn-detector`, for conversational turn-taking 
- `gpt-4o-transcribe` for speech synthesis
- `gpt-4o-mini` to generate responses
- `rime` to generate realistic text-to-speech (TTS)

By the end, you'll have a working voice agent that you can talk to in your browser. 

<img
  src="/images/livekit-agent-flow-light.png"
  alt="Voice agent architecture showing audio flowing from user through LiveKit, OpenAI, and Rime"
  className="block dark:hidden"
/>
<img
  src="/images/livekit-agent-flow-dark.png"
  alt="Voice agent architecture showing audio flowing from user through LiveKit, OpenAI, and Rime"
  className="hidden dark:block"
/>

If you'd like to experiment with Rime's TTS API directly before building a full voice agent, check out: [generate text-to-speech in under five minutes](/api-reference/quickstart-python).

## Prerequisites

To follow this guide you need:

- A **[Rime API Key](https://app.rime.ai/tokens/)** for text-to-speech synthesis
- An **[OpenAI API Key*](https://platform.openai.com/api-keys)** for speech-to-text (STT) and LLM responses (see [Alternative Providers](#alternative-providers) for using Deepgram and Gemini instead)
- A **[LiveKit Cloud Account](https://cloud.livekit.io/)** for real-time audio transport
    - Create a new LiveKit project called `rime-agent`
    - Go to **Settings** -> **API keys** -> **Create key** to generate a **WebSocket URL**, **API key**, and **API secret**. You will need all three of these
- A language runtime to put it all together:
  - **Python 3.10 or later**, or
  - **Node.js 18 or later**

## Project setup

Create a new folder for your project and navigate into it:

```bash
mkdir rime-voice-agent
cd rime-voice-agent
```

### Set up the environment variables

In the new directory, create a file called `.env` to store your API keys securely:

```bash
touch .env
```

Open `.env` and add your keys:

```
LIVEKIT_URL=wss://your-project.livekit.cloud
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
OPENAI_API_KEY=your_openai_api_key
RIME_API_KEY=your_rime_api_key
```

### Configure the dependencies

<Tabs>
<Tab title="Python" icon="python">

Install the Python package manager `uv` to handle dependencies:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Create a file called `pyproject.toml` and add the following to define your project and its dependencies:

```toml
[project]
name = "rime-voice-agent"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
    "livekit-agents[openai,rime,silero,turn-detector]~=1.2",
    "livekit-plugins-noise-cancellation~=0.2",
    "python-dotenv>=1.1.1",
]
```
This file adds the following:
- The `openai` plugin connects to OpenAI for STT and LLM
- **rime plugin**: Connects to Rime for text-to-speech
- **silero plugin**: Voice Activity Detection (knows when you start/stop speaking)
- **turn-detector plugin**: Detects when you've finished your turn in the conversation
- **noise-cancellation plugin**: Filters out background noise
- **python-dotenv** - Loads your API keys from the `.env` file

Now, start a virtual environment and download the dependencies by running:

```bash
uv sync
```

</Tab>
<Tab title="JavaScript" icon="js">

Initialize a Node.js project and install the LiveKit Agents SDK plus plugins:

```bash
npm init -y
npm pkg set type=module
npm install @livekit/agents \
  @livekit/agents-plugin-openai \
  @livekit/agents-plugin-rime \
  @livekit/agents-plugin-silero \
  dotenv
```

These packages add the following:
- **@livekit/agents**: The core LiveKit Agents SDK
- **openai plugin**: Connects to OpenAI for STT and LLM
- **rime plugin**: Connects to Rime for text-to-speech
- **silero plugin**: Voice Activity Detection (knows when you start/stop speaking)
- **dotenv**: Loads your API keys from the `.env` file

After installing, download the required model files:

```bash
node -e "import('@livekit/agents-plugin-silero').then(m => m.VAD.load())"
```

</Tab>
</Tabs>

## Create the agent

Now we'll walk you through building an agent from scratch, step-by-step. If you'd rather paste a working file and read along, grab the full code below. If you're happy with the defaults and just want to run it, skip ahead to testing.

- [Copy the full agent code](#full-agent-code)
- [Skip to testing](#test-your-agent)

### Full agent code

Create a file called `agent.py` or `agent.js` for all the code that gets your agent talking. Then, copy the following code:

<Accordion title="Full agent code (copy/paste)">
<CodeGroup>
```python Python
from dotenv import load_dotenv
from livekit.agents import (
    Agent,
    AgentSession,
    AutoSubscribe,
    JobContext,
    JobProcess,
    RoomInputOptions,
    WorkerOptions,
    cli,
)
from livekit.plugins import openai, noise_cancellation, rime, silero
from livekit.plugins.turn_detector.multilingual import MultilingualModel

load_dotenv()

class Assistant(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a helpful voice assistant.
            Keep your responses short and conversational - no more than 2-3 sentences.
            Be friendly and natural."""
        )

def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()

async def entrypoint(ctx: JobContext):
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    await ctx.wait_for_participant()

    session = AgentSession(
        stt=openai.STT(model="gpt-4o-transcribe"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=rime.TTS(model="arcana", speaker="luna"),
        vad=ctx.proc.userdata["vad"],
        turn_detection=MultilingualModel(),
    )

    await session.start(
        room=ctx.room,
        agent=Assistant(),
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC()
        ),
    )

    await session.say("Hey there! How can I help you today?")

if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
        ),
    )
```

```javascript JavaScript
import "dotenv/config";
import { cli, defineAgent, voice, WorkerOptions } from "@livekit/agents";
import * as openai from "@livekit/agents-plugin-openai";
import * as rime from "@livekit/agents-plugin-rime";
import * as silero from "@livekit/agents-plugin-silero";
import { fileURLToPath } from "node:url";

class Assistant extends voice.Agent {
    constructor() {
        super({
            instructions: `You are a helpful voice assistant.
            Keep your responses short and conversational - no more than 2-3 sentences.
            Be friendly and natural.`,
        });
    }
}

export default defineAgent({
    prewarm: async (proc) => {
        proc.userData.vad = await silero.VAD.load();
    },
    entry: async (ctx) => {
        await ctx.connect();

        const session = new voice.AgentSession({
            stt: new openai.STT({ model: "gpt-4o-transcribe" }),
            llm: new openai.LLM({ model: "gpt-4o-mini" }),
            tts: new rime.TTS({ model: "arcana", speaker: "luna" }),
            vad: ctx.proc.userData.vad,
        });

        await session.start({
            agent: new Assistant(),
            room: ctx.room,
        });

        await session.say("Hey there! How can I help you today?");
    },
});

cli.runApp(new WorkerOptions({ agent: fileURLToPath(import.meta.url) }));
```
</CodeGroup>
</Accordion>

### Load environment variables

First, load the environment variables so you can use them throughout the application:

<CodeGroup>
```python Python
from dotenv import load_dotenv

load_dotenv()
```

```javascript JavaScript
import "dotenv/config";
```
</CodeGroup>

### Define an Agent class
Now it's time to get stuck in with the LiveKit SDK.
Define your agent's personality by creating a class that extends the `voice.Agent` base class:

<CodeGroup>
```python Python
class Assistant(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a helpful voice assistant. 
            Keep your responses short and conversational - no more than 2-3 sentences.
            Be friendly and natural."""
        )
```

```javascript JavaScript
class Assistant extends voice.Agent {
    constructor() {
        super({
            instructions: `You are a helpful voice assistant.
            Keep your responses short and conversational - no more than 2-3 sentences.
            Be friendly and natural.`,
        });
    }
}
```
</CodeGroup>

Initialize the class by giving it a system prompt to guide your agent's responses and personality. This prompt can be as simple or complex as you like. Later in the guide (see [Fine-tune agent personalities](#fine-tune-agent-personalities)) you will see an example of a detailed system prompt that will fully customize the agent's behavior.

Remember to import the LiveKit Agents SDK and plugins. For convenience, you can find the complete list of required imports for this guide below:

<CodeGroup>
```python Python
from dotenv import load_dotenv
from livekit.agents import (
    Agent,
    AgentSession,
    AutoSubscribe,
    JobContext,
    JobProcess,
    RoomInputOptions,
    WorkerOptions,
    cli,
)
from livekit.plugins import openai, noise_cancellation, rime, silero
from livekit.plugins.turn_detector.multilingual import MultilingualModel
```

```javascript JavaScript
import "dotenv/config";
import { cli, defineAgent, voice, WorkerOptions } from "@livekit/agents";
import * as openai from "@livekit/agents-plugin-openai";
import * as rime from "@livekit/agents-plugin-rime";
import * as silero from "@livekit/agents-plugin-silero";
import { fileURLToPath } from "node:url";
```
</CodeGroup>
### Code the conversation pipeline 

The entry function runs each time a user connects. First, connect to the LiveKit room:

<CodeGroup>
```python Python
async def entrypoint(ctx: JobContext):
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    
    await ctx.wait_for_participant()
```

```javascript JavaScript
export default defineAgent({
    entry: async (ctx) => {
        await ctx.connect();
        // ... rest of the entry function
    },
});
```
</CodeGroup>

Next, in the same function, create the `AgentSession` (or `voice.AgentSession` in JavaScript) which wires together all the components of your voice pipeline. This is where you specify which models the agent uses for each stage:

<CodeGroup>
```python Python  
    session = AgentSession(
        stt=openai.STT(model="gpt-4o-transcribe"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=rime.TTS(model="arcana", speaker="luna"),
        vad=ctx.proc.userdata["vad"],
        turn_detection=MultilingualModel(),
    )
```

```javascript JavaScript
        const session = new voice.AgentSession({
            stt: new openai.STT({ model: "gpt-4o-transcribe" }),
            llm: new openai.LLM({ model: "gpt-4o-mini" }),
            tts: new rime.TTS({ model: "arcana", speaker: "luna" }),
            vad: ctx.proc.userData.vad,
        });
```
</CodeGroup>

Continue the function by starting the session and attaching it to the room. The noise cancellation option filters out background noise from the user's microphone (Python only - not yet available for Node.js):

<CodeGroup>
```python Python
    await session.start(
        room=ctx.room,
        agent=Assistant(),
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC()
        ),
    )
```

```javascript JavaScript
        await session.start({
            agent: new Assistant(),
            room: ctx.room,
        });
```
</CodeGroup>

Finally, configure the agent to start the session with a greeting:

<CodeGroup>
```python Python
    await session.say("Hey there! How can I help you today?")
```

```javascript JavaScript
        await session.say("Hey there! How can I help you today?");
```
</CodeGroup>
### Initialize the VAD plugin

Create a `prewarm` function that loads the Voice Activity Detection plugin once when the worker starts. We break the `prewarm` into its own function to avoid calling it for each new conversation:

<CodeGroup>
```python Python
def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()
```

```javascript JavaScript
export default defineAgent({
    prewarm: async (proc) => {
        proc.userData.vad = await silero.VAD.load();
    },
    // ... entry function
});
```
</CodeGroup>

### Create the main entrypoint

Finally, create the `__main__` block that runs when we execute the script:

<CodeGroup>
```python Python
if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
        ),
    )
```

```javascript JavaScript
cli.runApp(new WorkerOptions({ agent: fileURLToPath(import.meta.url) }));
```
</CodeGroup>

The `cli.run_app`/`cli.runApp` call starts the agent server that listens for incoming connections from LiveKit. In Python, `WorkerOptions` is configured with the two functions you created above: `prewarm_fnc` runs once per worker process to preload models, and `entrypoint_fnc` runs each time a user connects to a room. In JavaScript, you use `defineAgent` with `prewarm` and `entry` functions, and pass the agent file path to `WorkerOptions`.

## Test your agent

Start your agent by running:

<CodeGroup>
```bash Python
uv run python agent.py dev
```

```bash JavaScript
node agent.js dev
```
</CodeGroup>

The `dev` argument starts the agent in development mode. You'll see output like:

```
INFO   livekit.agents     starting worker {"version": "1.3.12", ...}
INFO   livekit.agents     initializing process {"pid": 12345, ...}
INFO   livekit.agents     process initialized
INFO   livekit.agents     HTTP server listening on :12332
```

### Connect to your agent

Open the [LiveKit Agents Playground](https://agents-playground.livekit.io) in your browser.

- Select your project and click **Use [project_name]**
- In the top right of the Playground, click **Connect**
- Allow microphone access when prompted

If everything is set up correctly, you should hear your agent say the greeting that you configured above:

<CodeGroup>
```python Python
    await session.say("Hey there! How can I help you today?")
```

```javascript JavaScript
    await session.say("Hey there! How can I help you today?");
```
</CodeGroup>

You can now talk to your agent using your microphone or by typing in the chat section.

### Change the voice

Rime offers many voices. Update the `tts` line to try different ones:

<CodeGroup>
```python Python
tts=rime.TTS(model="arcana", speaker="orion"),
```

```javascript JavaScript
tts: new rime.TTS({ model: "arcana", speaker: "orion" }),
```
</CodeGroup>

See more of the available voices on the [Voices](/api-reference/voices) page.

### Fine-tune agent personalities

For more complex personalities, store your system prompt in a separate file. Create a new file called `personality.py` or `personality.js`. See the example below for inspiration for how you could customize your agent:

<Accordion title="Example personality file">
<CodeGroup>
```python Python
SYSTEM_PROMPT = """
CHARACTER:
You are Gary, an overly enthusiastic tech support agent who is convinced 
that every problem can be solved by turning it off and on again. You work 
for a fictional company called "Reboot Solutions" and take your job very seriously.

PERSONALITY:
- Extremely optimistic, even when things are clearly going wrong
- Obsessed with the power of rebooting
- Uses tech jargon incorrectly but confidently
- Gets genuinely excited when users describe their problems
- Occasionally references "the great outage of 2019" as a formative experience

SPEECH STYLE:
- Keep responses to 2-3 sentences maximum
- Add natural fillers like "um", "well", "you know"
- Express enthusiasm with phrases like "Oh, fantastic!" and "Love it!"
- Always end with a question to keep the conversation going

RESTRICTIONS:
- Never break character
- Don't use emojis or special characters
- Avoid technical accuracy - confidence matters more than correctness
"""

INTRO_MESSAGE = "Oh hey there! Welcome to Reboot Solutions, I'm Gary! What can I help you restart today?"
```

```javascript JavaScript
export const SYSTEM_PROMPT = `
CHARACTER:
You are Gary, an overly enthusiastic tech support agent who is convinced
that every problem can be solved by turning it off and on again. You work
for a fictional company called "Reboot Solutions" and take your job very seriously.

PERSONALITY:
- Extremely optimistic, even when things are clearly going wrong
- Obsessed with the power of rebooting
- Uses tech jargon incorrectly but confidently
- Gets genuinely excited when users describe their problems
- Occasionally references "the great outage of 2019" as a formative experience

SPEECH STYLE:
- Keep responses to 2-3 sentences maximum
- Add natural fillers like "um", "well", "you know"
- Express enthusiasm with phrases like "Oh, fantastic!" and "Love it!"
- Always end with a question to keep the conversation going

RESTRICTIONS:
- Never break character
- Don't use emojis or special characters
- Avoid technical accuracy - confidence matters more than correctness
`;

export const INTRO_MESSAGE =
    "Oh hey there! Welcome to Reboot Solutions, I'm Gary! What can I help you restart today?";
```
</CodeGroup>
</Accordion>

Then update your `agent.py` or `agent.js` to import and use this prompt:

<CodeGroup>
```python Python
from personality import SYSTEM_PROMPT, INTRO_MESSAGE

class Assistant(Agent):
    def __init__(self):
        super().__init__(instructions=SYSTEM_PROMPT)
```

```javascript JavaScript
import { SYSTEM_PROMPT, INTRO_MESSAGE } from "./personality.js";

class Assistant extends voice.Agent {
    constructor() {
        super({ instructions: SYSTEM_PROMPT });
    }
}
```
</CodeGroup>

And update the greeting at the end of `entrypoint`:

<CodeGroup>
```python Python
await session.say(INTRO_MESSAGE)
```

```javascript JavaScript
await session.say(INTRO_MESSAGE);
```
</CodeGroup>

This approach keeps your personality configuration separate from your agent logic, making it easy to experiment with different characters.

## Alternative providers

LiveKit's plugin system lets you swap out your models for other STT and LLM providers. For example, if you don't have an OpenAI subscription, here's how to use Deepgram for STT and Google Gemini for the LLM.

### Update dependencies

Replace the OpenAI plugin with Deepgram and Google plugins.

<CodeGroup>
```toml Python
dependencies = [
    "livekit-agents[deepgram,google,rime,silero,turn-detector]~=1.2",
    "livekit-plugins-noise-cancellation~=0.2",
    "python-dotenv>=1.1.1",
]
```

```bash JavaScript
npm install @livekit/agents-plugin-deepgram @livekit/agents-plugin-google
```
</CodeGroup>

Run `uv sync` (Python) or `npm install` (JavaScript) to install the new dependencies.

### Update Environment Variables

Replace `OPENAI_API_KEY` in your `.env` file with:

```
DEEPGRAM_API_KEY=your_deepgram_api_key
GOOGLE_API_KEY=your_google_api_key
```

Get your keys from:
- **Deepgram**: [console.deepgram.com](https://console.deepgram.com/)
- **Google AI Studio**: [aistudio.google.com/apikey](https://aistudio.google.com/)

Don't change your LiveKit or Rime variables.

### Update the agent code

Change the imports and `AgentSession` configuration in `agent.py` or `agent.js`:

<CodeGroup>
```python Python
from livekit.plugins import deepgram, google, noise_cancellation, rime, silero
```

```javascript JavaScript
import * as deepgram from "@livekit/agents-plugin-deepgram";
import * as google from "@livekit/agents-plugin-google";
```
</CodeGroup>

<CodeGroup>
```python Python
session = AgentSession(
    stt=deepgram.STT(),
    llm=google.LLM(model="gemini-2.0-flash"),
    tts=rime.TTS(model="arcana", speaker="luna"),
    vad=ctx.proc.userdata["vad"],
    turn_detection=MultilingualModel(),
)
```

```javascript JavaScript
const session = new voice.AgentSession({
    stt: new deepgram.STT(),
    llm: new google.LLM({ model: "gemini-2.0-flash" }),
    tts: new rime.TTS({ model: "arcana", speaker: "luna" }),
    vad: ctx.proc.userData.vad,
});
```
</CodeGroup>

Your agent now uses Deepgram for transcription and Gemini for responses while keeping Rime for natural-sounding speech.

## Troubleshooting

### Agent doesn't respond to speech

- **Check microphone permissions:** Ensure your browser has microphone access enabled.
- **Verify VAD is working:** Look for `speech detected` logs in the terminal. If missing, check your Silero plugin installation.
- **Test with text input:** Use the chat input in the Playground to confirm the agent logic works.

### "Connection refused" or agent won't start

- **Check environment variables:** Ensure all keys in `.env` are set correctly with no extra spaces.
- **Verify LiveKit credentials:** Confirm your `LIVEKIT_URL`, `LIVEKIT_API_KEY`, and `LIVEKIT_API_SECRET` match your LiveKit Cloud project.

### Incorrect voice detection

- **Enable noise cancellation (Python only):** Verify `noise_cancellation.BVC()` is included in your `RoomInputOptions`.
- **Check your microphone:** Test with a different input device or headset
- **Reduce background noise:** The VAD may struggle to detect speech in noisy environments.
